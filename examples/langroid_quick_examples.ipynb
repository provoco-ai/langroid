{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/langroid/langroid/blob/main/examples/langroid_quick_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "<img width=\"700\" src=\"https://raw.githubusercontent.com/langroid/langroid/main/docs/assets/langroid-card-lambda-ossem-rust-1200-630.png\" alt=\"Langroid\">\n",
        "\n",
        "# Overview\n",
        "\n",
        "This notebook provides the runnable code for the six [**Usage Examples**](https://github.com/langroid/langroid#tada-usage-examples) described in [Langroid repo](https://github.com/langroid/langroid).\n",
        "\n",
        "**NOTE:** Notebooks (colab, jupyter, or otherwise) are *not* an ideal way to run interactive chat loops. We are showing these examples here since we recognize that Colab notebooks offer the benefit of having a ready to run environment with minimal setup. But we encourage you to try the python scripts in the [examples folder](https://github.com/langroid/langroid/tree/main/examples) of the repo on the command line for the best experience.\n",
        "\n",
        "In the first two cells we show the steps for setting up the requirements to run the examples including the installation of `Langroid` package and setting up the `OPENAI_API_KEY`.\n"
      ],
      "metadata": {
        "id": "uIV7QkkrC8O7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Langroid\n",
        "\n",
        "At the end there may be a message saying \"RESTART RUNTIME\", which can be safely ignored."
      ],
      "metadata": {
        "id": "hoTp_cNcriIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langroid[hf-embeddings]"
      ],
      "metadata": {
        "id": "PYaFworprwEJ",
        "outputId": "10aea924-05b2-4fe2-af0a-21c34ffa202c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langroid[hf-embeddings]\n",
            "  Downloading langroid-0.1.108-py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.8/177.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-generator<2.0,>=1.10 (from langroid[hf-embeddings])\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting autopep8<3.0.0,>=2.0.2 (from langroid[hf-embeddings])\n",
            "  Downloading autopep8-2.0.4-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.3/45.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black[jupyter]<24.0.0,>=23.3.0 (from langroid[hf-embeddings])\n",
            "  Downloading black-23.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bs4<0.0.2,>=0.0.1 (from langroid[hf-embeddings])\n",
            "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting chromadb<0.4.0,>=0.3.21 (from langroid[hf-embeddings])\n",
            "  Downloading chromadb-0.3.29-py3-none-any.whl (396 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog<7.0.0,>=6.7.0 (from langroid[hf-embeddings])\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting docstring-parser<0.16,>=0.15 (from langroid[hf-embeddings])\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Collecting faker<19.0.0,>=18.9.0 (from langroid[hf-embeddings])\n",
            "  Downloading Faker-18.13.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fakeredis<3.0.0,>=2.12.1 (from langroid[hf-embeddings])\n",
            "  Downloading fakeredis-2.20.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1 (from langroid[hf-embeddings])\n",
            "  Downloading farm_haystack-1.21.2-py3-none-any.whl (819 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.7/819.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fire<0.6.0,>=0.5.0 (from langroid[hf-embeddings])\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flake8<7.0.0,>=6.0.0 (from langroid[hf-embeddings])\n",
            "  Downloading flake8-6.1.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting google-api-python-client<3.0.0,>=2.95.0 (from langroid[hf-embeddings])\n",
            "  Downloading google_api_python_client-2.106.0-py2.py3-none-any.whl (12.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.6/12.6 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting halo<0.0.32,>=0.0.31 (from langroid[hf-embeddings])\n",
            "  Downloading halo-0.0.31.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from langroid[hf-embeddings]) (3.1.2)\n",
            "Collecting lancedb<0.4.0,>=0.3.0 (from langroid[hf-embeddings])\n",
            "  Downloading lancedb-0.3.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting litellm<0.7.0,>=0.6.2 (from langroid[hf-embeddings])\n",
            "  Downloading litellm-0.6.6-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml<5.0.0,>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from langroid[hf-embeddings]) (4.9.3)\n",
            "Collecting meilisearch<0.29.0,>=0.28.3 (from langroid[hf-embeddings])\n",
            "  Downloading meilisearch-0.28.4-py3-none-any.whl (21 kB)\n",
            "Collecting meilisearch-python-sdk<3.0.0,>=2.0.1 (from langroid[hf-embeddings])\n",
            "  Downloading meilisearch_python_sdk-2.0.1-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mkdocs<2.0.0,>=1.4.2 (from langroid[hf-embeddings])\n",
            "  Downloading mkdocs-1.5.3-py3-none-any.whl (3.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mkdocs-awesome-pages-plugin<3.0.0,>=2.8.0 (from langroid[hf-embeddings])\n",
            "  Downloading mkdocs_awesome_pages_plugin-2.9.2-py3-none-any.whl (14 kB)\n",
            "Collecting mkdocs-gen-files<0.5.0,>=0.4.0 (from langroid[hf-embeddings])\n",
            "  Downloading mkdocs_gen_files-0.4.0-py3-none-any.whl (8.1 kB)\n",
            "Collecting mkdocs-jupyter<0.25.0,>=0.24.1 (from langroid[hf-embeddings])\n",
            "  Downloading mkdocs_jupyter-0.24.6-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mkdocs-literate-nav<0.7.0,>=0.6.0 (from langroid[hf-embeddings])\n",
            "  Downloading mkdocs_literate_nav-0.6.1-py3-none-any.whl (13 kB)\n",
            "Collecting mkdocs-material<10.0.0,>=9.1.5 (from langroid[hf-embeddings])\n",
            "  Downloading mkdocs_material-9.4.7-py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mkdocs-rss-plugin<2.0.0,>=1.8.0 (from langroid[hf-embeddings])\n",
            "  Downloading mkdocs_rss_plugin-1.8.0-py2.py3-none-any.whl (19 kB)\n",
            "Collecting mkdocs-section-index<0.4.0,>=0.3.5 (from langroid[hf-embeddings])\n",
            "  Downloading mkdocs_section_index-0.3.8-py3-none-any.whl (8.7 kB)\n",
            "Collecting mkdocstrings[python]<0.22.0,>=0.21.2 (from langroid[hf-embeddings])\n",
            "  Downloading mkdocstrings-0.21.2-py3-none-any.whl (26 kB)\n",
            "Collecting momento<2.0.0,>=1.10.2 (from langroid[hf-embeddings])\n",
            "  Downloading momento-1.12.0-py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mypy<2.0.0,>=1.2.0 (from langroid[hf-embeddings])\n",
            "  Downloading mypy-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from langroid[hf-embeddings]) (3.8.1)\n",
            "Collecting openai<0.28.0,>=0.27.5 (from langroid[hf-embeddings])\n",
            "  Downloading openai-0.27.10-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas<3.0.0,>=2.0.3 (from langroid[hf-embeddings])\n",
            "  Downloading pandas-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfplumber<0.11.0,>=0.10.2 (from langroid[hf-embeddings])\n",
            "  Downloading pdfplumber-0.10.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pre-commit<4.0.0,>=3.3.2 (from langroid[hf-embeddings])\n",
            "  Downloading pre_commit-3.5.0-py2.py3-none-any.whl (203 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.7/203.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prettytable<4.0.0,>=3.8.0 in /usr/local/lib/python3.10/dist-packages (from langroid[hf-embeddings]) (3.9.0)\n",
            "Collecting pydantic==1.10.11 (from langroid[hf-embeddings])\n",
            "  Downloading pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygithub<2.0.0,>=1.58.1 (from langroid[hf-embeddings])\n",
            "  Downloading PyGithub-1.59.1-py3-none-any.whl (342 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.2/342.2 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pygments<3.0.0,>=2.15.1 in /usr/local/lib/python3.10/dist-packages (from langroid[hf-embeddings]) (2.16.1)\n",
            "Collecting pymupdf<2.0.0,>=1.23.3 (from langroid[hf-embeddings])\n",
            "  Downloading PyMuPDF-1.23.5-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing<4.0.0,>=3.0.9 in /usr/local/lib/python3.10/dist-packages (from langroid[hf-embeddings]) (3.1.1)\n",
            "Collecting pypdf<4.0.0,>=3.12.2 (from langroid[hf-embeddings])\n",
            "  Downloading pypdf-3.17.0-py3-none-any.whl (277 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m277.4/277.4 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytest-asyncio<0.22.0,>=0.21.1 (from langroid[hf-embeddings])\n",
            "  Downloading pytest_asyncio-0.21.1-py3-none-any.whl (13 kB)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.0 (from langroid[hf-embeddings])\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting qdrant-client<2.0.0,>=1.3.1 (from langroid[hf-embeddings])\n",
            "  Downloading qdrant_client-1.6.4-py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rank-bm25<0.3.0,>=0.2.2 (from langroid[hf-embeddings])\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Collecting redis<5.0.0,>=4.5.5 (from langroid[hf-embeddings])\n",
            "  Downloading redis-4.6.0-py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.1/241.1 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from langroid[hf-embeddings]) (2.31.0)\n",
            "Requirement already satisfied: requests-oauthlib<2.0.0,>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from langroid[hf-embeddings]) (1.3.1)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.3.4 in /usr/local/lib/python3.10/dist-packages (from langroid[hf-embeddings]) (13.6.0)\n",
            "Collecting ruff<0.0.271,>=0.0.270 (from langroid[hf-embeddings])\n",
            "  Downloading ruff-0.0.270-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scrapy<3.0.0,>=2.11.0 (from langroid[hf-embeddings])\n",
            "  Downloading Scrapy-2.11.0-py2.py3-none-any.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.4/286.4 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy<3.0.0,>=2.0.19 in /usr/local/lib/python3.10/dist-packages (from langroid[hf-embeddings]) (2.0.22)\n",
            "Collecting tantivy==0.20.1 (from langroid[hf-embeddings])\n",
            "  Downloading tantivy-0.20.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting thefuzz<0.21.0,>=0.20.0 (from langroid[hf-embeddings])\n",
            "  Downloading thefuzz-0.20.0-py3-none-any.whl (15 kB)\n",
            "Collecting tiktoken<0.6.0,>=0.5.1 (from langroid[hf-embeddings])\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trafilatura<2.0.0,>=1.5.0 (from langroid[hf-embeddings])\n",
            "  Downloading trafilatura-1.6.2-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer<0.10.0,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from langroid[hf-embeddings]) (0.9.0)\n",
            "Collecting types-redis<5.0.0.0,>=4.5.5.2 (from langroid[hf-embeddings])\n",
            "  Downloading types_redis-4.6.0.8-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-requests<3.0.0.0,>=2.31.0.1 (from langroid[hf-embeddings])\n",
            "  Downloading types_requests-2.31.0.10-py3-none-any.whl (14 kB)\n",
            "Collecting unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16 (from langroid[hf-embeddings])\n",
            "  Downloading unstructured-0.10.16-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wget<4.0,>=3.2 (from langroid[hf-embeddings])\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentence-transformers==2.2.2 (from langroid[hf-embeddings])\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch==2.0.0 (from langroid[hf-embeddings])\n",
            "  Downloading torch-2.0.0-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.11->langroid[hf-embeddings]) (4.5.0)\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers==2.2.2->langroid[hf-embeddings])\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->langroid[hf-embeddings]) (4.66.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->langroid[hf-embeddings]) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->langroid[hf-embeddings]) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->langroid[hf-embeddings]) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2->langroid[hf-embeddings]) (1.11.3)\n",
            "Collecting sentencepiece (from sentence-transformers==2.2.2->langroid[hf-embeddings])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence-transformers==2.2.2->langroid[hf-embeddings])\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->langroid[hf-embeddings]) (3.12.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->langroid[hf-embeddings]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.0.0->langroid[hf-embeddings]) (3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.0->langroid[hf-embeddings])\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.0->langroid[hf-embeddings])\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.0->langroid[hf-embeddings])\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.0->langroid[hf-embeddings])\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.0->langroid[hf-embeddings])\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.0->langroid[hf-embeddings])\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.0->langroid[hf-embeddings])\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.0->langroid[hf-embeddings])\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.0->langroid[hf-embeddings])\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.0->langroid[hf-embeddings])\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.0->langroid[hf-embeddings])\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==2.0.0 (from torch==2.0.0->langroid[hf-embeddings])\n",
            "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->langroid[hf-embeddings]) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.0->langroid[hf-embeddings]) (0.41.2)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch==2.0.0->langroid[hf-embeddings]) (3.27.7)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.0->langroid[hf-embeddings])\n",
            "  Downloading lit-17.0.4.tar.gz (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.1/153.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycodestyle>=2.10.0 (from autopep8<3.0.0,>=2.0.2->langroid[hf-embeddings])\n",
            "  Downloading pycodestyle-2.11.1-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from autopep8<3.0.0,>=2.0.2->langroid[hf-embeddings]) (2.0.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]<24.0.0,>=23.3.0->langroid[hf-embeddings]) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black[jupyter]<24.0.0,>=23.3.0->langroid[hf-embeddings])\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: packaging>=22.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]<24.0.0,>=23.3.0->langroid[hf-embeddings]) (23.2)\n",
            "Collecting pathspec>=0.9.0 (from black[jupyter]<24.0.0,>=23.3.0->langroid[hf-embeddings])\n",
            "  Downloading pathspec-0.11.2-py3-none-any.whl (29 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]<24.0.0,>=23.3.0->langroid[hf-embeddings]) (3.11.0)\n",
            "Requirement already satisfied: ipython>=7.8.0 in /usr/local/lib/python3.10/dist-packages (from black[jupyter]<24.0.0,>=23.3.0->langroid[hf-embeddings]) (7.34.0)\n",
            "Collecting tokenize-rt>=3.2.0 (from black[jupyter]<24.0.0,>=23.3.0->langroid[hf-embeddings])\n",
            "  Downloading tokenize_rt-5.2.0-py2.py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4<0.0.2,>=0.0.1->langroid[hf-embeddings]) (4.11.2)\n",
            "Collecting hnswlib>=0.7 (from chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading hnswlib-0.7.0.tar.gz (33 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading clickhouse_connect-0.6.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (954 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.4/954.4 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings]) (0.8.1)\n",
            "Collecting fastapi==0.85.1 (from chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading fastapi-0.85.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading onnxruntime-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers>=0.13.2 (from chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting overrides>=7.3.1 (from chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Collecting starlette==0.20.4 (from fastapi==0.85.1->chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette==0.20.4->fastapi==0.85.1->chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings]) (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker<19.0.0,>=18.9.0->langroid[hf-embeddings]) (2.8.2)\n",
            "Requirement already satisfied: sortedcontainers<3,>=2 in /usr/local/lib/python3.10/dist-packages (from fakeredis<3.0.0,>=2.12.1->langroid[hf-embeddings]) (2.4.0)\n",
            "Collecting boilerpy3 (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading boilerpy3-1.0.6-py3-none-any.whl (22 kB)\n",
            "Collecting events (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading Events-0.5-py3-none-any.whl (6.8 kB)\n",
            "Collecting httpx (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading httpx-0.25.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings]) (4.19.1)\n",
            "Collecting lazy-imports==0.3.1 (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings]) (10.1.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings]) (9.4.0)\n",
            "Collecting prompthub-py==4.0.0 (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading prompthub_py-4.0.0-py3-none-any.whl (6.9 kB)\n",
            "Collecting quantulum3 (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading quantulum3-0.9.0-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-cache<1.0.0 (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading requests_cache-0.9.8-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn (from sentence-transformers==2.2.2->langroid[hf-embeddings])\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sseclient-py (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings]) (8.2.3)\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence-transformers==2.2.2->langroid[hf-embeddings])\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-ai-formrecognizer>=3.2.0b2 (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading azure_ai_formrecognizer-3.3.1-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.7/299.7 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings]) (3.5)\n",
            "Collecting python-docx (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading python_docx-1.0.1-py3-none-any.whl (237 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.4/237.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-frontmatter (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading python_frontmatter-1.0.0-py3-none-any.whl (9.0 kB)\n",
            "Collecting python-magic (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Collecting tika (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdf2image>1.14 (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Collecting pytesseract>0.3.7 (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Collecting langdetect (from farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from prompthub-py==4.0.0->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->langroid[hf-embeddings]) (2023.6.3)\n",
            "Collecting tokenizers>=0.13.2 (from chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2->langroid[hf-embeddings])\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire<0.6.0,>=0.5.0->langroid[hf-embeddings]) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire<0.6.0,>=0.5.0->langroid[hf-embeddings]) (2.3.0)\n",
            "Collecting mccabe<0.8.0,>=0.7.0 (from flake8<7.0.0,>=6.0.0->langroid[hf-embeddings])\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting pyflakes<3.2.0,>=3.1.0 (from flake8<7.0.0,>=6.0.0->langroid[hf-embeddings])\n",
            "  Downloading pyflakes-3.1.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httplib2<1.dev0,>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<3.0.0,>=2.95.0->langroid[hf-embeddings]) (0.22.0)\n",
            "Requirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<3.0.0,>=2.95.0->langroid[hf-embeddings]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<3.0.0,>=2.95.0->langroid[hf-embeddings]) (0.1.1)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<3.0.0,>=2.95.0->langroid[hf-embeddings]) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client<3.0.0,>=2.95.0->langroid[hf-embeddings]) (4.1.1)\n",
            "Collecting log_symbols>=0.0.14 (from halo<0.0.32,>=0.0.31->langroid[hf-embeddings])\n",
            "  Downloading log_symbols-0.0.14-py3-none-any.whl (3.1 kB)\n",
            "Collecting spinners>=0.0.24 (from halo<0.0.32,>=0.0.31->langroid[hf-embeddings])\n",
            "  Downloading spinners-0.0.24-py3-none-any.whl (5.5 kB)\n",
            "Collecting colorama>=0.3.9 (from halo<0.0.32,>=0.0.31->langroid[hf-embeddings])\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<4.0.0,>=3.1.2->langroid[hf-embeddings]) (2.1.3)\n",
            "Collecting deprecation (from lancedb<0.4.0,>=0.3.0->langroid[hf-embeddings])\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pylance==0.8.7 (from lancedb<0.4.0,>=0.3.0->langroid[hf-embeddings])\n",
            "  Downloading pylance-0.8.7-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ratelimiter~=1.0 (from lancedb<0.4.0,>=0.3.0->langroid[hf-embeddings])\n",
            "  Downloading ratelimiter-1.2.0.post0-py3-none-any.whl (6.6 kB)\n",
            "Collecting retry>=0.9.2 (from lancedb<0.4.0,>=0.3.0->langroid[hf-embeddings])\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from lancedb<0.4.0,>=0.3.0->langroid[hf-embeddings]) (3.8.6)\n",
            "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from lancedb<0.4.0,>=0.3.0->langroid[hf-embeddings]) (23.1.0)\n",
            "Collecting semver>=3.0 (from lancedb<0.4.0,>=0.3.0->langroid[hf-embeddings])\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from lancedb<0.4.0,>=0.3.0->langroid[hf-embeddings]) (5.3.2)\n",
            "Collecting pyarrow>=10 (from pylance==0.8.7->lancedb<0.4.0,>=0.3.0->langroid[hf-embeddings])\n",
            "  Downloading pyarrow-13.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs<2.0.0,>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from litellm<0.7.0,>=0.6.2->langroid[hf-embeddings]) (1.4.4)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.10/dist-packages (from litellm<0.7.0,>=0.6.2->langroid[hf-embeddings]) (6.8.0)\n",
            "Collecting camel-converter[pydantic] (from meilisearch<0.29.0,>=0.28.3->langroid[hf-embeddings])\n",
            "  Downloading camel_converter-3.1.0-py3-none-any.whl (5.6 kB)\n",
            "Requirement already satisfied: PyJWT>=2.3.0 in /usr/lib/python3/dist-packages (from meilisearch-python-sdk<3.0.0,>=2.0.1->langroid[hf-embeddings]) (2.3.0)\n",
            "Collecting aiofiles>=0.7 (from meilisearch-python-sdk<3.0.0,>=2.0.1->langroid[hf-embeddings])\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting ghp-import>=1.0 (from mkdocs<2.0.0,>=1.4.2->langroid[hf-embeddings])\n",
            "  Downloading ghp_import-2.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting mergedeep>=1.3.4 (from mkdocs<2.0.0,>=1.4.2->langroid[hf-embeddings])\n",
            "  Downloading mergedeep-1.3.4-py3-none-any.whl (6.4 kB)\n",
            "Collecting pyyaml-env-tag>=0.1 (from mkdocs<2.0.0,>=1.4.2->langroid[hf-embeddings])\n",
            "  Downloading pyyaml_env_tag-0.1-py3-none-any.whl (3.9 kB)\n",
            "Collecting watchdog>=2.0 (from mkdocs<2.0.0,>=1.4.2->langroid[hf-embeddings])\n",
            "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: natsort>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from mkdocs-awesome-pages-plugin<3.0.0,>=2.8.0->langroid[hf-embeddings]) (8.4.0)\n",
            "Collecting wcmatch>=7 (from mkdocs-awesome-pages-plugin<3.0.0,>=2.8.0->langroid[hf-embeddings])\n",
            "  Downloading wcmatch-8.5-py3-none-any.whl (39 kB)\n",
            "Collecting ipykernel<7.0.0,>6.0.0 (from mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings])\n",
            "  Downloading ipykernel-6.26.0-py3-none-any.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.3/114.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jupytext<2,>1.13.8 (from mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings])\n",
            "  Downloading jupytext-1.15.2-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nbconvert<8,>=7.2.9 (from mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings])\n",
            "  Downloading nbconvert-7.10.0-py3-none-any.whl (256 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.7/256.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: babel~=2.10 in /usr/local/lib/python3.10/dist-packages (from mkdocs-material<10.0.0,>=9.1.5->langroid[hf-embeddings]) (2.13.1)\n",
            "Collecting mkdocs-material-extensions~=1.3 (from mkdocs-material<10.0.0,>=9.1.5->langroid[hf-embeddings])\n",
            "  Downloading mkdocs_material_extensions-1.3-py3-none-any.whl (8.6 kB)\n",
            "Collecting paginate~=0.5 (from mkdocs-material<10.0.0,>=9.1.5->langroid[hf-embeddings])\n",
            "  Downloading paginate-0.5.6.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymdown-extensions~=10.2 (from mkdocs-material<10.0.0,>=9.1.5->langroid[hf-embeddings])\n",
            "  Downloading pymdown_extensions-10.3.1-py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.0/241.0 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython<3.2,>=3.1 (from mkdocs-rss-plugin<2.0.0,>=1.8.0->langroid[hf-embeddings])\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mkdocs-autorefs>=0.3.1 (from mkdocstrings[python]<0.22.0,>=0.21.2->langroid[hf-embeddings])\n",
            "  Downloading mkdocs_autorefs-0.5.0-py3-none-any.whl (9.6 kB)\n",
            "Collecting mkdocstrings-python>=0.5.2 (from mkdocstrings[python]<0.22.0,>=0.21.2->langroid[hf-embeddings])\n",
            "  Downloading mkdocstrings_python-1.7.3-py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.7/51.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio<2.0.0,>=1.46.0 in /usr/local/lib/python3.10/dist-packages (from momento<2.0.0,>=1.10.2->langroid[hf-embeddings]) (1.59.0)\n",
            "Collecting momento-wire-types<0.87.0,>=0.86.0 (from momento<2.0.0,>=1.10.2->langroid[hf-embeddings])\n",
            "  Downloading momento_wire_types-0.86.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyJWT>=2.3.0 (from meilisearch-python-sdk<3.0.0,>=2.0.1->langroid[hf-embeddings])\n",
            "  Downloading PyJWT-2.8.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->langroid[hf-embeddings]) (1.3.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.0.3->langroid[hf-embeddings]) (2023.3.post1)\n",
            "Collecting tzdata>=2022.1 (from pandas<3.0.0,>=2.0.3->langroid[hf-embeddings])\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20221105 (from pdfplumber<0.11.0,>=0.10.2->langroid[hf-embeddings])\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdfium2>=4.18.0 (from pdfplumber<0.11.0,>=0.10.2->langroid[hf-embeddings])\n",
            "  Downloading pypdfium2-4.23.1-py3-none-manylinux_2_17_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber<0.11.0,>=0.10.2->langroid[hf-embeddings]) (3.3.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber<0.11.0,>=0.10.2->langroid[hf-embeddings]) (41.0.5)\n",
            "Collecting cfgv>=2.0.0 (from pre-commit<4.0.0,>=3.3.2->langroid[hf-embeddings])\n",
            "  Downloading cfgv-3.4.0-py2.py3-none-any.whl (7.2 kB)\n",
            "Collecting identify>=1.0.0 (from pre-commit<4.0.0,>=3.3.2->langroid[hf-embeddings])\n",
            "  Downloading identify-2.5.31-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.9/98.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nodeenv>=0.11.1 (from pre-commit<4.0.0,>=3.3.2->langroid[hf-embeddings])\n",
            "  Downloading nodeenv-1.8.0-py2.py3-none-any.whl (22 kB)\n",
            "Collecting virtualenv>=20.10.0 (from pre-commit<4.0.0,>=3.3.2->langroid[hf-embeddings])\n",
            "  Downloading virtualenv-20.24.6-py3-none-any.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable<4.0.0,>=3.8.0->langroid[hf-embeddings]) (0.2.8)\n",
            "Collecting deprecated (from pygithub<2.0.0,>=1.58.1->langroid[hf-embeddings])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting pynacl>=1.4.0 (from pygithub<2.0.0,>=1.58.1->langroid[hf-embeddings])\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.5 (from pymupdf<2.0.0,>=1.23.3->langroid[hf-embeddings])\n",
            "  Downloading PyMuPDFb-1.23.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytest>=7.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest-asyncio<0.22.0,>=0.21.1->langroid[hf-embeddings]) (7.4.3)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.3.1->langroid[hf-embeddings])\n",
            "  Downloading grpcio_tools-1.59.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker<3.0.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.3.1->langroid[hf-embeddings])\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting urllib3<2.0.0,>=1.26.14 (from qdrant-client<2.0.0,>=1.3.1->langroid[hf-embeddings])\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: async-timeout>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from redis<5.0.0,>=4.5.5->langroid[hf-embeddings]) (4.0.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->langroid[hf-embeddings]) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->langroid[hf-embeddings]) (2023.7.22)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib<2.0.0,>=1.3.1->langroid[hf-embeddings]) (3.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=13.3.4->langroid[hf-embeddings]) (3.0.0)\n",
            "Collecting Twisted<23.8.0,>=18.9.0 (from scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cssselect>=0.9.1 (from scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting itemloaders>=1.0.1 (from scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
            "Collecting parsel>=1.5.0 (from scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings]) (23.2.0)\n",
            "Collecting queuelib>=1.4.2 (from scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
            "Collecting service-identity>=18.1.0 (from scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading service_identity-23.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting w3lib>=1.17.0 (from scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading w3lib-2.1.2-py3-none-any.whl (21 kB)\n",
            "Collecting zope.interface>=5.1.0 (from scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading zope.interface-6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.1/247.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protego>=0.1.15 (from scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading Protego-0.3.0-py2.py3-none-any.whl (8.5 kB)\n",
            "Collecting itemadapter>=0.1.0 (from scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
            "Collecting tldextract (from scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading tldextract-5.0.1-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.1/97.1 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyDispatcher>=2.0.5 (from scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3.0.0,>=2.0.19->langroid[hf-embeddings]) (3.0.0)\n",
            "Collecting rapidfuzz<4.0.0,>=3.0.0 (from thefuzz<0.21.0,>=0.20.0->langroid[hf-embeddings])\n",
            "  Downloading rapidfuzz-3.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting courlan>=0.9.4 (from trafilatura<2.0.0,>=1.5.0->langroid[hf-embeddings])\n",
            "  Downloading courlan-0.9.4-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting htmldate>=1.5.1 (from trafilatura<2.0.0,>=1.5.0->langroid[hf-embeddings])\n",
            "  Downloading htmldate-1.5.2-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting justext>=3.0.0 (from trafilatura<2.0.0,>=1.5.0->langroid[hf-embeddings])\n",
            "  Downloading jusText-3.0.0-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.8/837.8 kB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-pyOpenSSL (from types-redis<5.0.0.0,>=4.5.5.2->langroid[hf-embeddings])\n",
            "  Downloading types_pyOpenSSL-23.3.0.0-py3-none-any.whl (7.2 kB)\n",
            "INFO: pip is looking at multiple versions of types-requests to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.1 (from langroid[hf-embeddings])\n",
            "  Downloading types_requests-2.31.0.9-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.8-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.7-py3-none-any.whl (14 kB)\n",
            "  Downloading types_requests-2.31.0.6-py3-none-any.whl (14 kB)\n",
            "Collecting types-urllib3 (from types-requests<3.0.0.0,>=2.31.0.1->langroid[hf-embeddings])\n",
            "  Downloading types_urllib3-1.26.25.14-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings]) (5.2.0)\n",
            "Collecting filetype (from unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings]) (0.9.0)\n",
            "Collecting emoji (from unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading emoji-2.8.0-py2.py3-none-any.whl (358 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m358.9/358.9 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dataclasses-json (from unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting python-iso639 (from unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading python_iso639-2023.6.15-py3-none-any.whl (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.1/275.1 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-pptx<=0.6.21 (from unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading python-pptx-0.6.21.tar.gz (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting unstructured-inference (from unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading unstructured_inference-0.7.10-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unstructured.pytesseract>=0.3.12 (from unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading unstructured.pytesseract-0.3.12-py3-none-any.whl (14 kB)\n",
            "Collecting azure-core<2.0.0,>=1.23.0 (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading azure_core-1.29.5-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msrest>=0.6.21 (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-common~=1.1 (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect>=0.5.7->chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading lz4-4.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langcodes>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from courlan>=0.9.4->trafilatura<2.0.0,>=1.5.0->langroid[hf-embeddings]) (3.3.0)\n",
            "Collecting tld>=0.13 (from courlan>=0.9.4->trafilatura<2.0.0,>=1.5.0->langroid[hf-embeddings])\n",
            "  Downloading tld-0.13-py2.py3-none-any.whl (263 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.8/263.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber<0.11.0,>=0.10.2->langroid[hf-embeddings]) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython<3.2,>=3.1->mkdocs-rss-plugin<2.0.0,>=1.8.0->langroid[hf-embeddings])\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.95.0->langroid[hf-embeddings]) (1.61.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.95.0->langroid[hf-embeddings]) (3.20.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client<3.0.0,>=2.95.0->langroid[hf-embeddings]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client<3.0.0,>=2.95.0->langroid[hf-embeddings]) (4.9)\n",
            "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client<3.0.0,>=2.95.0->langroid[hf-embeddings])\n",
            "  Downloading protobuf-4.24.4-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio<2.0.0,>=1.46.0 (from momento<2.0.0,>=1.10.2->langroid[hf-embeddings])\n",
            "  Downloading grpcio-1.59.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dateparser>=1.1.2 (from htmldate>=1.5.1->trafilatura<2.0.0,>=1.5.0->langroid[hf-embeddings])\n",
            "  Downloading dateparser-1.1.8-py2.py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpcore<0.19.0,>=0.18.0 (from httpx->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading httpcore-0.18.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings]) (1.3.0)\n",
            "Collecting h2<5,>=3 (from httpx->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2->langroid[hf-embeddings]) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.8.0->litellm<0.7.0,>=0.6.2->langroid[hf-embeddings]) (3.17.0)\n",
            "Collecting comm>=0.1.1 (from ipykernel<7.0.0,>6.0.0->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings])\n",
            "  Downloading comm-0.1.4-py3-none-any.whl (6.6 kB)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>6.0.0->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (1.6.6)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>6.0.0->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (6.1.12)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>6.0.0->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (5.4.0)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>6.0.0->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (0.1.6)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>6.0.0->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (1.5.8)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>6.0.0->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=20 in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>6.0.0->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (23.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>6.0.0->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (6.3.2)\n",
            "Requirement already satisfied: traitlets>=5.4.0 in /usr/local/lib/python3.10/dist-packages (from ipykernel<7.0.0,>6.0.0->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (5.7.1)\n",
            "Collecting jedi>=0.16 (from ipython>=7.8.0->black[jupyter]<24.0.0,>=23.3.0->langroid[hf-embeddings])\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]<24.0.0,>=23.3.0->langroid[hf-embeddings]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]<24.0.0,>=23.3.0->langroid[hf-embeddings]) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]<24.0.0,>=23.3.0->langroid[hf-embeddings]) (3.0.39)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]<24.0.0,>=23.3.0->langroid[hf-embeddings]) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=7.8.0->black[jupyter]<24.0.0,>=23.3.0->langroid[hf-embeddings]) (4.8.0)\n",
            "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from jupytext<2,>1.13.8->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (5.9.2)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from jupytext<2,>1.13.8->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (0.10.2)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from jupytext<2,>1.13.8->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (0.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.3.4->langroid[hf-embeddings]) (0.1.2)\n",
            "Collecting griffe>=0.35 (from mkdocstrings-python>=0.5.2->mkdocstrings[python]<0.22.0,>=0.21.2->langroid[hf-embeddings])\n",
            "  Downloading griffe-0.36.9-py3-none-any.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8,>=7.2.9->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert<8,>=7.2.9->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert<8,>=7.2.9->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (0.2.2)\n",
            "Collecting mistune<4,>=2.0.3 (from nbconvert<8,>=7.2.9->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings])\n",
            "  Downloading mistune-3.0.2-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.0/48.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8,>=7.2.9->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (0.8.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8,>=7.2.9->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert<8,>=7.2.9->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (1.2.1)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings]) (23.5.26)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=7.0.0->pytest-asyncio<0.22.0,>=0.21.1->langroid[hf-embeddings]) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=7.0.0->pytest-asyncio<0.22.0,>=0.21.1->langroid[hf-embeddings]) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=7.0.0->pytest-asyncio<0.22.0,>=0.21.1->langroid[hf-embeddings]) (1.1.3)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<=0.6.21->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cattrs>=22.2 (from requests-cache<1.0.0->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading cattrs-23.1.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting url-normalize>=1.4 (from requests-cache<1.0.0->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry>=0.9.2->lancedb<0.4.0,>=0.3.0->langroid[hf-embeddings])\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2->langroid[hf-embeddings]) (3.2.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings]) (0.5.0)\n",
            "Collecting constantly>=15.1 (from Twisted<23.8.0,>=18.9.0->scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
            "Collecting incremental>=21.3.0 (from Twisted<23.8.0,>=18.9.0->scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
            "Collecting Automat>=0.8.0 (from Twisted<23.8.0,>=18.9.0->scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
            "Collecting hyperlink>=17.1.1 (from Twisted<23.8.0,>=18.9.0->scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting distlib<1,>=0.3.7 (from virtualenv>=20.10.0->pre-commit<4.0.0,>=3.3.2->langroid[hf-embeddings])\n",
            "  Downloading distlib-0.3.7-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.9/468.9 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bracex>=2.1.1 (from wcmatch>=7->mkdocs-awesome-pages-plugin<3.0.0,>=2.8.0->langroid[hf-embeddings])\n",
            "  Downloading bracex-2.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lancedb<0.4.0,>=0.3.0->langroid[hf-embeddings]) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lancedb<0.4.0,>=0.3.0->langroid[hf-embeddings]) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lancedb<0.4.0,>=0.3.0->langroid[hf-embeddings]) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->lancedb<0.4.0,>=0.3.0->langroid[hf-embeddings]) (1.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4<0.0.2,>=0.0.1->langroid[hf-embeddings]) (2.5)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->pygithub<2.0.0,>=1.58.1->langroid[hf-embeddings]) (1.14.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings]) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings]) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings]) (0.10.6)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from quantulum3->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings]) (7.0.0)\n",
            "Collecting num2words (from quantulum3->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.0.0->langroid[hf-embeddings]) (1.3.0)\n",
            "Collecting requests-file>=1.4 (from tldextract->scrapy<3.0.0,>=2.11.0->langroid[hf-embeddings])\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from sentence-transformers==2.2.2->langroid[hf-embeddings])\n",
            "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading torchvision-0.15.1-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting layoutparser[layoutmodels,tesseract] (from unstructured-inference->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart (from unstructured-inference->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opencv-python!=4.7.0.68 in /usr/local/lib/python3.10/dist-packages (from unstructured-inference->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings]) (4.8.0.76)\n",
            "Collecting onnx (from unstructured-inference->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading onnxruntime-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.2.0 (from pydantic==1.10.11->langroid[hf-embeddings])\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach!=5.0.0->nbconvert<8,>=7.2.9->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (0.5.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber<0.11.0,>=0.10.2->langroid[hf-embeddings]) (2.21)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.2->htmldate>=1.5.1->trafilatura<2.0.0,>=1.5.0->langroid[hf-embeddings]) (5.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython<3.2,>=3.1->mkdocs-rss-plugin<2.0.0,>=1.8.0->langroid[hf-embeddings])\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=7.8.0->black[jupyter]<24.0.0,>=23.3.0->langroid[hf-embeddings]) (0.8.3)\n",
            "Collecting isodate>=0.6.0 (from msrest>=0.6.21->azure-ai-formrecognizer>=3.2.0b2->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->jupytext<2,>1.13.8->mkdocs-jupyter<0.25.0,>=0.24.1->langroid[hf-embeddings]) (2.18.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=7.8.0->black[jupyter]<24.0.0,>=23.3.0->langroid[hf-embeddings]) (0.7.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb<0.4.0,>=0.3.21->langroid[hf-embeddings])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting iopath (from layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting effdet (from layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2 (from num2words->quantulum3->farm-haystack[file-conversion,ocr,pdf,preprocessing]<2.0.0,>=1.21.1->langroid[hf-embeddings])\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting timm>=0.9.2 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading timm-0.9.8-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings]) (2.0.7)\n",
            "Collecting omegaconf>=2.0 (from effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings]) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings]) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings]) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->effdet->layoutparser[layoutmodels,tesseract]->unstructured-inference->unstructured[docx,pdf,pptx]<0.10.18,>=0.10.16->langroid[hf-embeddings]) (1.4.5)\n",
            "Building wheels for collected packages: sentence-transformers, bs4, fire, halo, wget, hnswlib, paginate, python-pptx, langdetect, tika, lit, docopt, iopath, antlr4-python3-runtime\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=d4a36331cf3b1c1fc684b3c80479c2f87486f2314622996539def9b053903bd1\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for bs4 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1256 sha256=ee15ad6b0cd010c3203a0351121b20efe781c39a3d1b21a233e14686edf57308\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/42/45/b773edc52acb16cd2db4cf1a0b47117e2f69bb4eb300ed0e70\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116934 sha256=822a6fb2a828235390164431807c575692d84b04d6edb6d624727c93d2ad0476\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "  Building wheel for halo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for halo: filename=halo-0.0.31-py3-none-any.whl size=11235 sha256=ccf8b2f13ae08ecbe0f122aa984bae6595718ec39959c96835d0e5b006034446\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/d9/8a/b4f14c44aba7c164d4379eca6f1dde59360050406b1edaec24\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=d57689c0eaf3c1feed89ac98d226efa5d7cf6075c86eaf99856cc9620e773d3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.7.0-cp310-cp310-linux_x86_64.whl size=2202685 sha256=5fcd1e9457796e43137274539cb863ec2146a3ecca4025a352b576b9ef36c1ad\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/ae/ec/235a682e0041fbaeee389843670581ec6c66872db856dfa9a4\n",
            "  Building wheel for paginate (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paginate: filename=paginate-0.5.6-py3-none-any.whl size=12666 sha256=b0981b6b08579179536c6b41c5d61b0fcebce1197f3f8a6d0d343c69e52a903e\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/d3/18/0b5bebc873f29bea61fedece1e92cbcbef416839dfe5bd0eef\n",
            "  Building wheel for python-pptx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-pptx: filename=python_pptx-0.6.21-py3-none-any.whl size=470932 sha256=1a85caab0388e5ebad375d3f379ef473e10dbc1451579dd550eeedb1b39a1438\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/dd/74/01b3ec7256a0800b99384e9a0f7620e358afc3a51a59bf9b49\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=b3802032dbcb0f78491fba4ad4beb048d7ec55f6ed280554d20604d375d35e5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32621 sha256=5f495341ae134ec5f32d34f0636fe966053728fd3a3437ded993137fdfaa7d2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/71/c7/b757709531121b1700cffda5b6b0d4aad095fb507ec84316d0\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lit: filename=lit-17.0.4-py3-none-any.whl size=93257 sha256=1a31f59427d8839dcf658140943b95a4dbad4e16c16f7eb17e263717c762db10\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/ae/00/696c57d438bfc7c0e89c4c379083ea08b1c2e54d85a5f7cd7c\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=4fad2aea57e93e627e000e5ead3307554712778814eef9b23b7e00417905655e\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31530 sha256=75d6bc49fab63ffde434064efab59de537371ab6ccea00e0aec3064b3e704131\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=f1ee9eefce2ac866c1d0249d1fbd696ee52467dddf5759dcaa1de8239fe035d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built sentence-transformers bs4 fire halo wget hnswlib paginate python-pptx langdetect tika lit docopt iopath antlr4-python3-runtime\n",
            "Installing collected packages: wget, types-urllib3, tokenizers, sseclient-py, spinners, sentencepiece, ratelimiter, PyDispatcher, paginate, monotonic, lit, incremental, filetype, events, docopt, distlib, azure-common, antlr4-python3-runtime, zstandard, zope.interface, XlsxWriter, websockets, watchdog, w3lib, virtualenv, uvloop, urllib3, url-normalize, unstructured.pytesseract, tzdata, typing-extensions, types-requests, tokenize-rt, tld, tantivy, smmap, semver, safetensors, ruff, redis, rapidfuzz, rank-bm25, queuelib, pyyaml-env-tag, python-multipart, python-magic, python-iso639, python-frontmatter, python-dotenv, pytesseract, pypdfium2, pypdf, PyMuPDFb, pymdown-extensions, PyJWT, pyflakes, pycodestyle, pyarrow, py, pulsar-client, protobuf, protego, portalocker, pdf2image, pathspec, overrides, omegaconf, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, num2words, nodeenv, mypy-extensions, mkdocs-material-extensions, mistune, mergedeep, mccabe, marshmallow, lz4, lazy-imports, langdetect, justext, jmespath, jedi, itemadapter, isodate, identify, hyperlink, hyperframe, humanfriendly, httptools, hpack, hnswlib, h11, grpcio, fire, emoji, docstring-parser, deprecation, deprecated, cssselect, constantly, comm, colorlog, colorama, cfgv, camel-converter, bracex, boilerpy3, backoff, Automat, async-generator, aiofiles, wcmatch, watchfiles, uvicorn, typing-inspect, Twisted, thefuzz, starlette, scikit-learn, retry, python-pptx, python-docx, pytest-asyncio, pynacl, pymupdf, pylance, pydantic, pre-commit, parsel, pandas, onnx, nvidia-cusolver-cu11, nvidia-cudnn-cu11, mypy, momento-wire-types, log_symbols, iopath, httpcore, h2, grpcio-tools, griffe, gitdb, ghp-import, flake8, fakeredis, faker, dateparser, courlan, coloredlogs, clickhouse-connect, cattrs, bs4, black, autopep8, types-pyOpenSSL, tiktoken, tika, service-identity, requests-file, requests-cache, prompthub-py, posthog, pdfminer.six, openai, onnxruntime, momento, mkdocs, lancedb, itemloaders, ipykernel, huggingface-hub, httpx, htmldate, halo, GitPython, fastapi, dataclasses-json, azure-core, unstructured, types-redis, transformers, trafilatura, tldextract, quantulum3, pygithub, pdfplumber, msrest, mkdocs-section-index, mkdocs-rss-plugin, mkdocs-material, mkdocs-literate-nav, mkdocs-gen-files, mkdocs-awesome-pages-plugin, mkdocs-autorefs, meilisearch-python-sdk, meilisearch, litellm, google-api-python-client, chromadb, scrapy, qdrant-client, mkdocstrings, layoutparser, jupytext, farm-haystack, azure-ai-formrecognizer, nbconvert, mkdocstrings-python, mkdocs-jupyter, triton, torch, torchvision, timm, effdet, unstructured-inference, sentence-transformers, langroid\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: PyJWT\n",
            "    Found existing installation: PyJWT 2.3.0\n",
            "    Uninstalling PyJWT-2.3.0:\n",
            "      Successfully uninstalled PyJWT-2.3.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 9.0.0\n",
            "    Uninstalling pyarrow-9.0.0:\n",
            "      Successfully uninstalled pyarrow-9.0.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: mistune\n",
            "    Found existing installation: mistune 0.8.4\n",
            "    Uninstalling mistune-0.8.4:\n",
            "      Successfully uninstalled mistune-0.8.4\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.59.0\n",
            "    Uninstalling grpcio-1.59.0:\n",
            "      Successfully uninstalled grpcio-1.59.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.13\n",
            "    Uninstalling pydantic-1.10.13:\n",
            "      Successfully uninstalled pydantic-1.10.13\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 5.5.6\n",
            "    Uninstalling ipykernel-5.5.6:\n",
            "      Successfully uninstalled ipykernel-5.5.6\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.84.0\n",
            "    Uninstalling google-api-python-client-2.84.0:\n",
            "      Successfully uninstalled google-api-python-client-2.84.0\n",
            "  Attempting uninstall: nbconvert\n",
            "    Found existing installation: nbconvert 6.5.4\n",
            "    Uninstalling nbconvert-6.5.4:\n",
            "      Successfully uninstalled nbconvert-6.5.4\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.1.0\n",
            "    Uninstalling triton-2.1.0:\n",
            "      Successfully uninstalled triton-2.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu118\n",
            "    Uninstalling torch-2.1.0+cu118:\n",
            "      Successfully uninstalled torch-2.1.0+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu118\n",
            "    Uninstalling torchvision-0.16.0+cu118:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "google-colab 1.0.0 requires ipykernel==5.5.6, but you have ipykernel 6.26.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.2 which is incompatible.\n",
            "ibis-framework 6.2.0 requires pyarrow<13,>=2, but you have pyarrow 13.0.0 which is incompatible.\n",
            "pandas-gbq 0.17.9 requires pyarrow<10.0dev,>=3.0.0, but you have pyarrow 13.0.0 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.24.4 which is incompatible.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\n",
            "torchaudio 2.1.0+cu118 requires torch==2.1.0, but you have torch 2.0.0 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.0.0 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Automat-22.10.0 GitPython-3.1.40 PyDispatcher-2.0.7 PyJWT-2.8.0 PyMuPDFb-1.23.5 Twisted-22.10.0 XlsxWriter-3.1.9 aiofiles-23.2.1 antlr4-python3-runtime-4.9.3 async-generator-1.10 autopep8-2.0.4 azure-ai-formrecognizer-3.3.1 azure-common-1.1.28 azure-core-1.29.5 backoff-2.2.1 black-23.10.1 boilerpy3-1.0.6 bracex-2.4 bs4-0.0.1 camel-converter-3.1.0 cattrs-23.1.2 cfgv-3.4.0 chromadb-0.3.29 clickhouse-connect-0.6.18 colorama-0.4.6 coloredlogs-15.0.1 colorlog-6.7.0 comm-0.1.4 constantly-23.10.4 courlan-0.9.4 cssselect-1.2.0 dataclasses-json-0.6.1 dateparser-1.1.8 deprecated-1.2.14 deprecation-2.1.0 distlib-0.3.7 docopt-0.6.2 docstring-parser-0.15 effdet-0.4.1 emoji-2.8.0 events-0.5 faker-18.13.0 fakeredis-2.20.0 farm-haystack-1.21.2 fastapi-0.85.1 filetype-1.2.0 fire-0.5.0 flake8-6.1.0 ghp-import-2.1.0 gitdb-4.0.11 google-api-python-client-2.106.0 griffe-0.36.9 grpcio-1.59.2 grpcio-tools-1.59.2 h11-0.14.0 h2-4.1.0 halo-0.0.31 hnswlib-0.7.0 hpack-4.0.0 htmldate-1.5.2 httpcore-0.18.0 httptools-0.6.1 httpx-0.25.0 huggingface-hub-0.18.0 humanfriendly-10.0 hyperframe-6.0.1 hyperlink-21.0.0 identify-2.5.31 incremental-22.10.0 iopath-0.1.10 ipykernel-6.26.0 isodate-0.6.1 itemadapter-0.8.0 itemloaders-1.1.0 jedi-0.19.1 jmespath-1.0.1 jupytext-1.15.2 justext-3.0.0 lancedb-0.3.2 langdetect-1.0.9 langroid-0.1.108 layoutparser-0.3.4 lazy-imports-0.3.1 lit-17.0.4 litellm-0.6.6 log_symbols-0.0.14 lz4-4.3.2 marshmallow-3.20.1 mccabe-0.7.0 meilisearch-0.28.4 meilisearch-python-sdk-2.0.1 mergedeep-1.3.4 mistune-3.0.2 mkdocs-1.5.3 mkdocs-autorefs-0.5.0 mkdocs-awesome-pages-plugin-2.9.2 mkdocs-gen-files-0.4.0 mkdocs-jupyter-0.24.6 mkdocs-literate-nav-0.6.1 mkdocs-material-9.4.7 mkdocs-material-extensions-1.3 mkdocs-rss-plugin-1.8.0 mkdocs-section-index-0.3.8 mkdocstrings-0.21.2 mkdocstrings-python-1.7.3 momento-1.12.0 momento-wire-types-0.86.0 monotonic-1.6 msrest-0.7.1 mypy-1.6.1 mypy-extensions-1.0.0 nbconvert-7.10.0 nodeenv-1.8.0 num2words-0.5.13 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 omegaconf-2.3.0 onnx-1.15.0 onnxruntime-1.15.1 openai-0.27.10 overrides-7.4.0 paginate-0.5.6 pandas-2.1.2 parsel-1.8.1 pathspec-0.11.2 pdf2image-1.16.3 pdfminer.six-20221105 pdfplumber-0.10.3 portalocker-2.8.2 posthog-3.0.2 pre-commit-3.5.0 prompthub-py-4.0.0 protego-0.3.0 protobuf-4.24.4 pulsar-client-3.3.0 py-1.11.0 pyarrow-13.0.0 pycodestyle-2.11.1 pydantic-1.10.11 pyflakes-3.1.0 pygithub-1.59.1 pylance-0.8.7 pymdown-extensions-10.3.1 pymupdf-1.23.5 pynacl-1.5.0 pypdf-3.17.0 pypdfium2-4.23.1 pytesseract-0.3.10 pytest-asyncio-0.21.1 python-docx-1.0.1 python-dotenv-1.0.0 python-frontmatter-1.0.0 python-iso639-2023.6.15 python-magic-0.4.27 python-multipart-0.0.6 python-pptx-0.6.21 pyyaml-env-tag-0.1 qdrant-client-1.6.4 quantulum3-0.9.0 queuelib-1.6.2 rank-bm25-0.2.2 rapidfuzz-3.5.1 ratelimiter-1.2.0.post0 redis-4.6.0 requests-cache-0.9.8 requests-file-1.5.1 retry-0.9.2 ruff-0.0.270 safetensors-0.4.0 scikit-learn-1.3.2 scrapy-2.11.0 semver-3.0.2 sentence-transformers-2.2.2 sentencepiece-0.1.99 service-identity-23.1.0 smmap-5.0.1 spinners-0.0.24 sseclient-py-1.8.0 starlette-0.20.4 tantivy-0.20.1 thefuzz-0.20.0 tika-2.6.0 tiktoken-0.5.1 timm-0.9.8 tld-0.13 tldextract-5.0.1 tokenize-rt-5.2.0 tokenizers-0.13.3 torch-2.0.0 torchvision-0.15.1 trafilatura-1.6.2 transformers-4.32.1 triton-2.0.0 types-pyOpenSSL-23.3.0.0 types-redis-4.6.0.8 types-requests-2.31.0.6 types-urllib3-1.26.25.14 typing-extensions-4.8.0 typing-inspect-0.9.0 tzdata-2023.3 unstructured-0.10.16 unstructured-inference-0.7.10 unstructured.pytesseract-0.3.12 url-normalize-1.4.3 urllib3-1.26.18 uvicorn-0.23.2 uvloop-0.19.0 virtualenv-20.24.6 w3lib-2.1.2 watchdog-3.0.0 watchfiles-0.21.0 wcmatch-8.5 websockets-12.0 wget-3.2 zope.interface-6.1 zstandard-0.22.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up `OPENAI_API_KEY`\n",
        "\n",
        "This code will ask the user to provide the `OPENAI_API_KEY`. Before running this cell, please follow these steps to get the key.\n",
        "Login to your OpenAI account --> go to `View API Keys` from the drop-down list on the top-right corner --> click on the botton **create new secret key** --> a new screen will pop up --> press the botton **create secret key**.\n",
        "\n",
        "Visit [this page](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key) for more info about where to find the API Key."
      ],
      "metadata": {
        "id": "v-BiRXu9JQ5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "logging.getLogger().setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass('Enter your OPENAI_API_KEY key: ')"
      ],
      "metadata": {
        "id": "GOR8OsfvsN2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now you can can try any of the following examples. It is recommended to go through these in sequence, although the order does NOT matter.**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Tw5RzVKl3pUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Direct interaction with OpenAI LLM\n",
        "\n",
        "In this simple example, we are directly sending a message-sequence to the OpenAI `chatCompletion` API. Note that to have a multi-round converation we have to manually accumulate the dialog.\n",
        "\n",
        "First do various imports."
      ],
      "metadata": {
        "id": "5rBWNOuXEygx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langroid.language_models.base import LLMMessage, Role\n",
        "from langroid.language_models.openai_gpt import (\n",
        "        OpenAIGPTConfig,\n",
        "        OpenAIChatModel,\n",
        "        OpenAIGPT,\n",
        ")\n",
        "from langroid.language_models.base import LLMMessage, Role"
      ],
      "metadata": {
        "id": "lGY2XyHyD0oJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the API `OpenAIGPTConfig` to set the configurations of the OpenAI LLM model. We then define the LLM model using `OpenAIGPT`. We can also Specify the messages that will be sent to instruct the model.\n",
        "`Langroid` supports various roles provided by OpenAI.\n"
      ],
      "metadata": {
        "id": "bMR6Dani_l9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cfg = OpenAIGPTConfig(chat_model=OpenAIChatModel.GPT4) # or GPT_3_5_TURBO\n",
        "\n",
        "mdl = OpenAIGPT(cfg)\n",
        "\n",
        "messages = [\n",
        "  LLMMessage(content=\"You are a helpful assistant\",  role=Role.SYSTEM),\n",
        "  LLMMessage(content=\"What is the capital of Ontario?\",  role=Role.USER),\n",
        "]\n",
        "\n",
        "response = mdl.chat(messages, max_tokens=200)\n",
        "print(\"LLM response is:: \", response.message)\n",
        "\n",
        "# accumulate messages manually\n",
        "\n",
        "messages.append(response.to_LLMMessage())\n",
        "messages.append(LLMMessage(content=\"what about India?\", role=Role.USER))\n",
        "response = mdl.chat(messages, max_tokens=200)\n",
        "print(\"LLM response is:\", response.message)"
      ],
      "metadata": {
        "id": "JPul8c4uD1kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above is a \"raw\" LLM interaction where you have to manage\n",
        "message history. Using an Agent to wrap an LLM, and wrapping an Agent in a Task, we can set up an interactive, multi-round chat much more easily, as we show next."
      ],
      "metadata": {
        "id": "3HzZhfyAfRCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A note on the rest of the examples\n",
        "In the interactive examples below, the conversation loop pauses for human input: in most cases you would hit enter (unless the example requires you to ask a question).\n",
        "The interaction looks much better when run on a terminal,\n",
        "and a notebook is not ideal for these. However we realize a Colab notebook does offer the benefit of having a ready to run environment."
      ],
      "metadata": {
        "id": "4TjgYEfK34NZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define an agent, set up a task, and run it\n",
        "\n",
        "Say you want to have a multi-round interactive chat with an LLM.\n",
        "\n",
        "`Langroid` simplifies this process. We just need to create a `ChatAgent`, wrap it in a `Task`, and finally run the task."
      ],
      "metadata": {
        "id": "OaIiDn8zOurc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\n",
        "from langroid.agent.task import Task\n",
        "from langroid.language_models.openai_gpt import OpenAIChatModel, OpenAIGPTConfig"
      ],
      "metadata": {
        "id": "wFX7u9k-OyDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For constructing the `ChatAgent`, we need to set up some configurations:\n",
        "\n",
        "*   No vector database will be created\n",
        "*   chat model (`GPT4` or `GPT_3_5_TURBO`)\n",
        "\n",
        "Note that `Langroid` offers specialized chatting agents such as `DocChatAgent` and `TableChatAgent`, which we will see later."
      ],
      "metadata": {
        "id": "vaXKQWh-SBgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = ChatAgentConfig(\n",
        "    llm = OpenAIGPTConfig(\n",
        "        chat_model=OpenAIChatModel.GPT4,\n",
        "    ),\n",
        "    vecdb=None,\n",
        ")\n",
        "agent = ChatAgent(config)"
      ],
      "metadata": {
        "id": "PblTvXC6QZsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `ChatAgent` by itself offers 3 standard \"responders\": the LLM, the human User, and the Agent itself (e.g. to handle tool/function-calling by the LLM). To use these responders in an interactive loop, we need to wrap the Agent in a task,\n",
        "and call its `run()` method.\n",
        "\n",
        "A prompt will be displayed after running this task, so you can interact with the `ChatAgent`.\n",
        "\n",
        "Type your questions and the agent will provide the LLM responses. When done, type `q` to exit.\n"
      ],
      "metadata": {
        "id": "LO8O-7vZSoS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.message_history.clear()\n",
        "task = Task(agent, name=\"Bot\")\n",
        "task.set_color_log(enable=False)\n",
        "task.run()"
      ],
      "metadata": {
        "id": "wheZ7NJGO2X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Three communicating agents\n",
        "\n",
        "The above example involved a single `ChatAgent`, but in non-trivial applications, we will often find it easier to divide responsibilities among multiple agents, each with different skills and responsibilities.\n",
        "\n",
        "If you attempt to solve these with a single Agent, you would have to keep track of multiple conversation states and loops, and it quickly gets out of hand. Agents offer a way to solve complex tasks in a modular fashion. Moreover, specialized agents can be designed and tested in isolation, and then combined to solve various tasks.\n",
        "\n",
        "`Langroid` streamlines the process of setting up multiple agents and orchestrating their interaction. Here's a toy numerical example (this helps keep token costs low!). Imagine a task where we want to construct a series of numbers using the following rule to transform the current number $n$:\n",
        "- if $n$ is even, the next number is $n/2$\n",
        "- if $n$ is odd, the next number is $3n+1$.\n",
        "\n",
        "We can have 3 agents collaborate to produce this sequence.\n",
        "Given the current number $n$,\n",
        "- `repeater_agent` simply returns $n$,\n",
        "- `even_agent` specializes in handling even numbers, and returns $n/2$ if $n$ is even, else says \"DO-NOT-KNOW\"\n",
        "- `odd_agent` specializes in handling odd numbers, and returns $3*n+1$ if $n$ is odd, else says \"DO-NOT-KNOW\""
      ],
      "metadata": {
        "id": "B4q8ojyAQ_OV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langroid.utils.constants import NO_ANSWER\n",
        "from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\n",
        "from langroid.agent.task import Task\n",
        "from langroid.language_models.openai_gpt import OpenAIChatModel, OpenAIGPTConfig"
      ],
      "metadata": {
        "id": "oUbVybSuRIFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we define chat model that will be used by the agents:"
      ],
      "metadata": {
        "id": "_1aeRre35Ghd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = ChatAgentConfig(\n",
        "    llm = OpenAIGPTConfig(\n",
        "        chat_model=OpenAIChatModel.GPT4,\n",
        "    ),\n",
        "    vecdb = None,\n",
        ")"
      ],
      "metadata": {
        "id": "tAQIs76rRaF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we create the `repeater_agent` and define its corresponding task, which comprises the following settings:\n",
        "\n",
        "\n",
        "*   **Name**: name of the agent\n",
        "*   **llm_delegate**: whether to delegate control to LLM; conceptually, the \"controlling entity\" is the one \"seeking\" responses to its queries, and has a goal it is aiming to achieve. The \"controlling entity\" is either the LLM or the USER. (Note within a Task there is just one LLM, and all other entities are proxies of the \"User\" entity).\n",
        "*   **single_round**: If true, the task runs until one message by the controller and a subsequent response by the non-controller. If false, runs for the specified number of turns in `run`, or until `done()` is true.\n",
        "* **system_message**: provides instructions to the LLM."
      ],
      "metadata": {
        "id": "xpqzS-ozUowm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repeater_agent = ChatAgent(config)\n",
        "repeater_task = Task(\n",
        "    repeater_agent,\n",
        "    name = \"Repeater\",\n",
        "    system_message=\"\"\"\n",
        "    Your job is to repeat whatever number you receive.\n",
        "    \"\"\",\n",
        "    llm_delegate=True, # LLM takes charge of task\n",
        "    single_round=False,\n",
        ")"
      ],
      "metadata": {
        "id": "AI5oFOM2SLgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define our second agent `even_agent` and its task `even_task`. Notice it takes the same `config` that we previously created."
      ],
      "metadata": {
        "id": "zVJ-nbs08Ub5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "even_agent = ChatAgent(config)\n",
        "even_task = Task(\n",
        "    even_agent,\n",
        "    name = \"EvenHandler\",\n",
        "    system_message=f\"\"\"\n",
        "    You will be given a number.\n",
        "    If it is even, divide by 2 and say the result, nothing else.\n",
        "    If it is odd, say {NO_ANSWER}\n",
        "    \"\"\",\n",
        "    single_round=True,  # task done after 1 step() with valid response\n",
        ")"
      ],
      "metadata": {
        "id": "pgOFydVqRbPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we create the 3rd agent `odd_agent` and its task `odd_task`.\n"
      ],
      "metadata": {
        "id": "z-RAYL9S9XTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "odd_agent = ChatAgent(config)\n",
        "odd_task = Task(\n",
        "    odd_agent,\n",
        "    name = \"OddHandler\",\n",
        "    system_message=f\"\"\"\n",
        "    You will be given a number n.\n",
        "    If it is odd, return (n*3+1), say nothing else.\n",
        "    If it is even, say {NO_ANSWER}\n",
        "    \"\"\",\n",
        "    single_round=True,  # task done after 1 step() with valid response\n",
        ")"
      ],
      "metadata": {
        "id": "ukf79CqIScHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use `add_sub_task` to orchestrate the collaboration between the agents.  Specifically, `repeater_task` will act as the \"main\", and we add `even_task` and `odd_task` as\n",
        "subtasks. For more details see these [docs](https://langroid.github.io/langroid/quick-start/multi-agent-task-delegation/#task-collaboration-via-sub-tasks).\n",
        "\n",
        "\n",
        "Finally, we kickoff the task with a starting number 3, using `repeater_task.run(\"3\")`.\n",
        "\n",
        "Remember to keep hitting enter when it's the human's turn, and hit \"q\" to end the conversation."
      ],
      "metadata": {
        "id": "PbGYXCpR9uVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repeater_task.add_sub_task([even_task, odd_task])\n",
        "repeater_task.set_color_log(enable=False)\n",
        "repeater_task.run(\"3\")"
      ],
      "metadata": {
        "id": "0vy77Os8TAas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Tool/Function-calling example\n",
        "\n",
        "Here is a simple numerical example showcasing how `Langroid` supports tools/function-calling. For more details see these [doc pages](https://langroid.github.io/langroid/quick-start/chat-agent-tool/)\n",
        "\n",
        "Say the agent has a secret list of numbers, and we want the LLM to find the smallest number in the list. We want to give the LLM the ability to use a **probe** tool/function which takes a single number `n` as an argument. The tool handler method in the agent returns how many numbers in its list are at most `n`."
      ],
      "metadata": {
        "id": "t7c3qKvcTwTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langroid.agent.tool_message import ToolMessage\n",
        "from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\n",
        "from langroid.language_models.openai_gpt import OpenAIChatModel, OpenAIGPTConfig\n",
        "from langroid.agent.task import Task\n",
        "from langroid.utils.configuration import set_global, Settings\n",
        "from langroid.embedding_models.models import OpenAIEmbeddingsConfig"
      ],
      "metadata": {
        "id": "qMG5Jq7wUZRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use tools/function-calling in `Langroid`, we first **define** the tool as a subclass of `ToolMessage` to specify some details about the tool (e.g., name and parameters) and when it can be used/triggered:\n",
        "* **request**: is the name of the tool/function, as well as the name of the Agent method that \"handles\" the tool.\n",
        "* **purpose**: general description to give hints to LLM when this tool can be used\n",
        "* **number**: is a function-argument for the `probe` tool and its type is `int`"
      ],
      "metadata": {
        "id": "htqVMFU2pog6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProbeTool(ToolMessage):\n",
        "  request: str = \"probe\"\n",
        "  purpose: str = \"\"\"\n",
        "        To find how many numbers in my list are less than or equal to\n",
        "        the <number> you specify.\n",
        "        \"\"\" # note  <number> corresponds to the name of the tool's argument/parameter\n",
        "  number: int\n"
      ],
      "metadata": {
        "id": "3eIidMmZUgir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we create an agent `SpyGameAgent`, with a special method `probe` to handle the `probe` tool/function.\n",
        "Notice the argument of the `probe` method is an instance of the class `ProbeTool` that we created in the previous step."
      ],
      "metadata": {
        "id": "vBAw8kxkqa42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpyGameAgent(ChatAgent):\n",
        "  def __init__(self, config: ChatAgentConfig):\n",
        "    super().__init__(config)\n",
        "    self.numbers = [3, 4, 8, 11, 15, 25, 40, 80, 90] # agent's secret list\n",
        "\n",
        "  def probe(self, msg: ProbeTool) -> str:\n",
        "    # return how many numbers in self.numbers are less or equal to msg.number\n",
        "    return str(len([n for n in self.numbers if n <= msg.number]))"
      ],
      "metadata": {
        "id": "einGIzjQUyX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we instantiate the `SpyGameAgent` as an object `spy_game_agent`, and \"associate\" the `probe` tool with this agent, using the `enable_message` method of the `ChatAgent`.  We then wrap the `spy_game_agent` in a `Task` object, with instructions (`system_message`) on what it should aim for."
      ],
      "metadata": {
        "id": "pG2MJJN4yCHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "spy_game_agent = SpyGameAgent(\n",
        "    ChatAgentConfig(\n",
        "        llm = OpenAIGPTConfig(\n",
        "            chat_model=OpenAIChatModel.GPT4,\n",
        "        ),\n",
        "        vecdb=None, # no vector database needed\n",
        "    )\n",
        ")\n",
        "\n",
        "spy_game_agent.enable_message(ProbeTool)\n",
        "\n",
        "task = Task(\n",
        "        spy_game_agent,\n",
        "        name=\"Spy\",\n",
        "        system_message=\"\"\"\n",
        "            I have a list of numbers between 1 and 20.\n",
        "            Your job is to find the smallest of them.\n",
        "            To help with this, you can give me a number and I will\n",
        "            tell you how many of my numbers are equal or less than your number.\n",
        "            Once you have found the smallest number,\n",
        "            you can say DONE and report your answer.\n",
        "        \"\"\",\n",
        "    )"
      ],
      "metadata": {
        "id": "OM6Lk3uWVOCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the task.\n",
        "\n",
        "Remember to keep hitting enter when it's the human's turn, and hit \"q\" to end the conversation."
      ],
      "metadata": {
        "id": "aQ93n0kA_rM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spy_game_agent.message_history.clear()\n",
        "task.set_color_log(enable=False)\n",
        "task.run()"
      ],
      "metadata": {
        "id": "xAVoOcgsNWOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat with documents (file paths, URLs, etc)\n",
        "\n",
        "In the previous examples, the Agents did not use any external documents. In this example, we we set up an Agent that supports \"chatting\" with documents. Specifically, we use the `DocChatAgent` class to ask questions about a set of URLs.\n",
        "The `DocChatAgent` first ingests the contents of the websites specified by the URLs by chunking, embedding and indexing them into a vector database (`qdrant` by default). We then wrap the agent in a task and run it interactively.\n",
        "The user can ask questions and the LLM of the agent returns answers using Retrieval Augment Generation, with Evidence Citation.\n"
      ],
      "metadata": {
        "id": "Qmeh3zJTZeL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langroid.agent.special.doc_chat_agent import DocChatAgentConfig, DocChatAgent\n",
        "from langroid.language_models.openai_gpt import OpenAIChatModel, OpenAIGPTConfig\n",
        "from langroid.vector_store.qdrantdb import QdrantDBConfig\n",
        "from langroid.embedding_models.models import OpenAIEmbeddingsConfig\n",
        "from langroid.agent.task import Task"
      ],
      "metadata": {
        "id": "qMuSX-DjZjwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define the configuration of the `DocChatAgent`. The configurations include the path to access the documents, chat model settings, and vector-DB settings."
      ],
      "metadata": {
        "id": "w1ZcFRJu5K5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = DocChatAgentConfig(\n",
        "  doc_paths = [\n",
        "    \"https://en.wikipedia.org/wiki/Language_model\",\n",
        "    \"https://en.wikipedia.org/wiki/N-gram_language_model\",\n",
        "  ],\n",
        "  llm = OpenAIGPTConfig(\n",
        "    chat_model=OpenAIChatModel.GPT4,\n",
        "  ),\n",
        "  vecdb=QdrantDBConfig(\n",
        "                collection_name=\"docqa-chat-multi-extract\",\n",
        "                storage_path=\".qdrant/test1/\", # CHANGE THIS PATH IF YOU GET AN ERROR WHEN RE-RUNNING THE CELL\n",
        "        ),\n",
        ")\n",
        "\n",
        "agent = DocChatAgent(config)"
      ],
      "metadata": {
        "id": "tVEHeg9jZpl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we wrap the agent in a task, and run it.\n",
        "\n",
        "Remember to keep hitting enter when it's the human's turn, and hit \"q\" to end the conversation."
      ],
      "metadata": {
        "id": "UZyTx3vQN_I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.message_history.clear()\n",
        "task = Task(agent)\n",
        "task.set_color_log(enable=False)\n",
        "task.run()"
      ],
      "metadata": {
        "id": "OMFDp8WxaAI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tool/Function-calling to extract structured information from text\n",
        "\n",
        "Let's combine multi-agent interaction, Retrieval-Augmented Generation, and tools/function-calling, for a more realistic example. Suppose you want an agent to extract the key terms of a lease, from a lease document, as a nested JSON structure.\n",
        "This can be accomplished by instructing the LLM to use a specific tool.\n",
        "\n",
        "To simplify the solution, we separate the skills/responsibilities into two different Agents:\n",
        "- `LeaseExtractorAgent` has no access to the lease, and is responsible for gathering the key terms into a specific structured form\n",
        "- `DocChatAgent` has access to the lease and answers specific questions it receives from the `LeaseExtractorAgent`.\n"
      ],
      "metadata": {
        "id": "kZqX1J6qWhHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rich import print\n",
        "from pydantic import BaseModel, BaseSettings\n",
        "from typing import List\n",
        "import json\n",
        "\n",
        "from langroid.agent.special.doc_chat_agent import DocChatAgent, DocChatAgentConfig\n",
        "from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\n",
        "from langroid.vector_store.qdrantdb import QdrantDBConfig\n",
        "from langroid.agent.task import Task\n",
        "from langroid.agent.tool_message import ToolMessage\n",
        "from langroid.language_models.openai_gpt import OpenAIGPTConfig\n",
        "from langroid.utils.configuration import set_global, Settings\n",
        "from langroid.utils.logging import setup_colored_logging\n",
        "from langroid.utils.constants import NO_ANSWER\n",
        "from langroid.embedding_models.models import OpenAIEmbeddingsConfig"
      ],
      "metadata": {
        "id": "sKRoGAvqWzMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define the desired structure of the lease information via Pydantic models. The desired format is a nested JSON structure, which maps to a nested class structure:\n"
      ],
      "metadata": {
        "id": "d-CkI7wk5A8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeasePeriod(BaseModel):\n",
        "    start_date: str\n",
        "    end_date: str\n",
        "\n",
        "class LeaseFinancials(BaseModel):\n",
        "    monthly_rent: str\n",
        "    deposit: str\n",
        "\n",
        "class Lease(BaseModel):\n",
        "    \"\"\"\n",
        "    Various lease terms.\n",
        "    Nested fields to make this more interesting/realistic\n",
        "    \"\"\"\n",
        "\n",
        "    period: LeasePeriod\n",
        "    financials: LeaseFinancials\n",
        "    address: str"
      ],
      "metadata": {
        "id": "0p8iveEcX_1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then define the `LeaseMessage` tool as a subclass of Langroid's `ToolMessage`. The `LeaseMessage` class has a\n",
        "required argument `terms` of type `Lease`. The `classmethod` named `examples` is used to generate $k$-shot examples for the LLM when instructing it to extract information in the desired structured form (see a later cell below).\n"
      ],
      "metadata": {
        "id": "WcUQylLu5HFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeaseMessage(ToolMessage):\n",
        "    request: str = \"lease_info\" # maps to method of LeaseExtractorAgent\n",
        "    purpose: str = \"\"\"\n",
        "        Collect information about a Commercial Lease.\n",
        "        \"\"\"\n",
        "    terms: Lease\n",
        "\n",
        "    @classmethod\n",
        "    def examples(cls) -> List[\"LeaseMessage\"]:\n",
        "        return [\n",
        "            cls(\n",
        "                terms=Lease(\n",
        "                    period=LeasePeriod(start_date=\"2021-01-01\", end_date=\"2021-12-31\"),\n",
        "                    financials=LeaseFinancials(monthly_rent=\"$1000\", deposit=\"$1000\"),\n",
        "                    address=\"123 Main St, San Francisco, CA 94105\",\n",
        "                ),\n",
        "                result=\"\",\n",
        "            ),\n",
        "        ]\n",
        "\n"
      ],
      "metadata": {
        "id": "XFVCpL8jW7C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we define the `LeaseExtractorAgent` and add a method `least_info` to handle the tool/function-call `lease_info` defined in the tool `LeaseMessage`. In this case the handling is trivial: if the method receives a valid object of class `LeaseMessage`, it declares \"success\"."
      ],
      "metadata": {
        "id": "Lu03iGEaW0Ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeaseExtractorAgent(ChatAgent):\n",
        "    def __init__(self, config: ChatAgentConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "    def lease_info(self, message: LeaseMessage) -> str:\n",
        "        print(\n",
        "            f\"\"\"\n",
        "        DONE! Successfully extracted Lease Info:\n",
        "        {message.terms}\n",
        "        \"\"\"\n",
        "        )\n",
        "        return json.dumps(message.terms.dict())"
      ],
      "metadata": {
        "id": "ZlZ0UtqEXGdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain the lease.txt document that we want to parsed\n",
        "!wget https://github.com/langroid/langroid-examples/blob/main/examples/docqa/lease.txt"
      ],
      "metadata": {
        "id": "HtHXm0-UuBRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, set up an instance of `DocChatAgent`, point it to the lease document, equip it with a vector database, and instructions on how to answer questions based on extracts retrieved from the vector-store.\n"
      ],
      "metadata": {
        "id": "mdkxu37f7JuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_agent = DocChatAgent(\n",
        "        DocChatAgentConfig(\n",
        "            doc_paths = [\"lease.txt\"],\n",
        "            vecdb=QdrantDBConfig(\n",
        "                collection_name=\"docqa-chat-multi-extract\",\n",
        "                storage_path=\".data1/data1/\", # CHANGE PATH IF ERROR\n",
        "              ),\n",
        "            summarize_prompt= f\"\"\"\n",
        "                Use the provided extracts to answer the question.\n",
        "                If there's not enough information, respond with {NO_ANSWER}. Use only the\n",
        "                information in these extracts, even if your answer is factually incorrect,\n",
        "                and even if the answer contradicts other parts of the document. The only\n",
        "                important thing is that your answer is consistent with and supported by the\n",
        "                extracts. Compose your complete answer and cite all supporting sources on a\n",
        "                separate separate line as \"EXTRACTS:\".\n",
        "                Show each EXTRACT very COMPACTLY, i.e. only show a few words from\n",
        "                the start and end of the extract, for example:\n",
        "                EXTRACT: \"The world war started in ... Germany Surrendered\"\n",
        "                {{extracts}}\n",
        "                {{question}}\n",
        "                Answer:\n",
        "            \"\"\"\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "d8ynRPp6XUvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we wrap the `doc_agent` into a Task, with instructions on its role.\n"
      ],
      "metadata": {
        "id": "DISYqYVr8BQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "doc_task = Task(\n",
        "    doc_agent,\n",
        "    name=\"DocAgent\",\n",
        "    llm_delegate=False,\n",
        "    single_round=True,\n",
        "    system_message=\"\"\"You are an expert on Commercial Leases.\n",
        "    You will receive various questions about a Commercial\n",
        "    Lease contract, and your job is to answer them concisely in at most 2 sentences.\n",
        "    Please SUPPORT your answer with an actual EXTRACT from the lease,\n",
        "    showing only a few words from the  START and END of the extract.\n",
        "    \"\"\",\n",
        ")"
      ],
      "metadata": {
        "id": "NBLp0AEy74N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we instantiate the `lease_extractor_agent`, enable it to use and handle the `LeaseMessage` tool. Then we wrap the `lease_extractor_agent` into a Task, instructing it to gather information in the desired format, by asking questions one at a time. Note how the instruction contains `LeaseMessage.usage_example()`: this example is constructed from the `examples` classmethod above when the `LeaseMessage` was defined.\n"
      ],
      "metadata": {
        "id": "ZhRfOPiE9W-n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lease_extractor_agent = LeaseExtractorAgent(\n",
        "    ChatAgentConfig(\n",
        "        vecdb=None,\n",
        "        llm=OpenAIGPTConfig(),\n",
        "    )\n",
        ")\n",
        "\n",
        "lease_extractor_agent.enable_message(\n",
        "    LeaseMessage,\n",
        "    use=True,\n",
        "    handle=True,\n",
        "    force=False,\n",
        ")\n",
        "\n",
        "lease_task = Task(\n",
        "    lease_extractor_agent,\n",
        "    name=\"LeaseExtractorAgent\",\n",
        "    llm_delegate=True,\n",
        "    single_round=False,\n",
        "    system_message=f\"\"\"\n",
        "    You have to collect some information about a Commercial Lease, but you do not\n",
        "    have access to the lease itself.\n",
        "    You can ask me questions about the lease, ONE AT A TIME, I will answer each\n",
        "    question. You only need to collect info corresponding to the fields in this\n",
        "    example:\n",
        "    {LeaseMessage.usage_example()}\n",
        "    If some info cannot be found, fill in {NO_ANSWER}.\n",
        "    When you have collected this info, present it to me using the\n",
        "    'lease_info' function/tool.\n",
        "    \"\"\",\n",
        ")"
      ],
      "metadata": {
        "id": "VL2RYZUX7393"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we set up the `doc_task` as a subtask of the `lease_task` so that the `doc_agent` can respond to questions from the `lease_extractor_agent`.\n",
        " Now, the `lease_extractor_agent` will be asking questions about the lease and `doc_task` will provide the answers, citing evidence extracted from the lease. Once `lease_extractor_agent` collects all the terms of the lease as instructed, it will use the tool `LeaseMessage` to return this information.\n",
        "\n",
        " The next cell runs the `lease_task`. Remember to keep hitting enter when it's the human's turn, and hit \"q\" to end the conversation."
      ],
      "metadata": {
        "id": "XBdAcarpwnYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lease_extractor_agent.message_history.clear()\n",
        "lease_task.add_sub_task(doc_task)\n",
        "lease_task.set_color_log(enable=False)\n",
        "lease_task.run()"
      ],
      "metadata": {
        "id": "1Lt9FhjTFlh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat with tabular data (file paths, URLs, dataframes)\n",
        "\n",
        "Here is how `Langroid's` `TableChatAgent` can be used to chat with tabular data, which can be specified as a URL, file path or Pandas dataframe.\n",
        "\n",
        "The Agent's LLM generates Pandas code to answer the query, via function-calling (or tool/plugin), and the Agent's function-handling method executes the code and returns the answer"
      ],
      "metadata": {
        "id": "uWcH7HoFc-am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langroid.agent.special.table_chat_agent import TableChatAgent, TableChatAgentConfig\n",
        "from langroid.agent.task import Task\n",
        "from langroid.language_models.openai_gpt import OpenAIChatModel, OpenAIGPTConfig\n"
      ],
      "metadata": {
        "id": "jS06lshgdBv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up a `TableChatAgent` for a data file, URL or dataframe (Ensure the data table has a header row; the delimiter/separator is auto-detected):"
      ],
      "metadata": {
        "id": "tS8pDD2MdgPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset =  \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
        "# or dataset = \"/path/to/my/data.csv\"\n",
        "# or dataset = pd.read_csv(\"/path/to/my/data.csv\")\n",
        "agent = TableChatAgent(\n",
        "    config=TableChatAgentConfig(\n",
        "        data=dataset,\n",
        "        llm=OpenAIGPTConfig(\n",
        "            chat_model=OpenAIChatModel.GPT4,\n",
        "        ),\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "qDbA4VGmda10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's set up a task and run it in an interactive loop with the user:\n",
        "Based on `dataset`, you can ask the following question in the prompt:\n",
        "\n",
        "```\n",
        "What is the average alcohol content of wines with a quality rating above 7?\n",
        "```\n",
        "\n",
        "Remember to keep hitting enter when it's the human's turn, and hit \"q\" to end the conversation."
      ],
      "metadata": {
        "id": "em2hcy2Qd67T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.message_history.clear()\n",
        "task = Task(agent, name=\"DataAssistant\")\n",
        "task.set_color_log(enable=False)\n",
        "task.run()"
      ],
      "metadata": {
        "id": "nhDMm5ndd93W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}